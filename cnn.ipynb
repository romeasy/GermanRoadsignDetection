{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a cNN to detect roadsigns\n",
    "In order to process roadsigns in the autonomous car of the Freie UniversitÃ¤t, we want to train a convolutional (deep) neural network.\n",
    "\n",
    "This network is supposed to distinguish between different classes of signs (stop, attention, train crossing etc) and the final model will then be integrated to the autonomos ROS structure.\n",
    "\n",
    "This notebook shall download the dataset, read it in and then train the classifier. Afterwards, a validation of the training procedure will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib2, cStringIO, zipfile\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip'\n",
    "\n",
    "if not os.path.exists('GTSRB/Final_Training/Images'):\n",
    "    try:\n",
    "        remotezip = urllib2.urlopen(url)\n",
    "        zipinmemory = cStringIO.StringIO(remotezip.read())\n",
    "        zip = zipfile.ZipFile(zipinmemory)\n",
    "        zip.extractall('.')\n",
    "    except urllib2.HTTPError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTrafficSigns(rootpath):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    # loop over all 42 classes\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        gtReader.next() # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            images.append(plt.imread(prefix + row[0])) # the 1th column is the filename\n",
    "            labels.append(row[7]) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainImg, trainLabels = readTrafficSigns('GTSRB/Final_Training/Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 39209\n",
      "Number of training labels: 39209\n",
      "Largest Image Dimensions: (225, 243, 3)\n"
     ]
    }
   ],
   "source": [
    "print \"Number of training images: \" + str(len(trainImg))\n",
    "print \"Number of training labels: \" + str(len(trainLabels))\n",
    "maxShape = (0,0)\n",
    "maxPos = 0\n",
    "pos = 0\n",
    "for img in trainImg:\n",
    "    if np.prod(img.shape) > np.prod(maxShape):\n",
    "        maxShape = img.shape\n",
    "        maxPos = pos\n",
    "    pos += 1\n",
    "print \"Largest Image Dimensions: \" + str(maxShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print trainLabels[-1]\n",
    "number_of_classes = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all images the same size\n",
    "For a convolutional Neural Network to work, the images all need to have the same size.\n",
    "This can be done by padding with black pixels. Then, all of the images are of the size of the largest image.\n",
    "\n",
    "However, this approach may have drawbacks when it comes to the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0 images\n",
      "Done with 500 images\n",
      "(1000, 225, 243, 3)\n"
     ]
    }
   ],
   "source": [
    "train_set = []\n",
    "count = 0\n",
    "for img in trainImg[:1000]:\n",
    "    padded = np.zeros(maxShape)\n",
    "    x1 = int(padded.shape[0] / 2. - img.shape[0] / 2.)\n",
    "    y1 = int(padded.shape[1] / 2. - img.shape[1] / 2.)\n",
    "    padded[x1:img.shape[0]+x1, y1:img.shape[1]+y1] = img\n",
    "    train_set.append(padded)\n",
    "    if count % 500 == 0:\n",
    "        print \"Done with \" + str(count) + \" images\"\n",
    "    count += 1\n",
    "print np.array(train_set).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train_set)\n",
    "for img in train_set:\n",
    "    if not img.shape == maxShape:\n",
    "        print \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform labels to one-hot-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "train_labels = []\n",
    "for label in trainLabels:\n",
    "    new_label = np.zeros(number_of_classes)\n",
    "    new_label[int(label)] = 1\n",
    "    train_labels.append(new_label)\n",
    "train_labels = np.array(train_labels)\n",
    "print train_labels[13451]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build the convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class implements a convolutional neural network classifier.\n",
    "Main usage should consist of two steps, namely train and evaluate. During training, the weights of the network\n",
    "will be changed in a way to nicely represent the data and classify it in the end.\n",
    "\n",
    "Parameters:\n",
    "    img_shape:\n",
    "        Shape of the images that will be presented to the\n",
    "        network as (width, height, #channels)\n",
    "    \n",
    "    learning_rate:\n",
    "        The learning rate used for gradient descent\n",
    "    \n",
    "    architecture:\n",
    "        List. It contains for each layer the number\n",
    "        of neurons. For the convolution, the number\n",
    "        of neurons corresponds to the number of kernels.\n",
    "\"\"\"\n",
    "class cNN:\n",
    "    \n",
    "    def __init__(self, architecture, img_shape, kernel_shape, learning_rate):\n",
    "        if len(architecture) > 5:\n",
    "            print \"ERROR. The network is too deep. So far, we can't deal with more than 5 layers!\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.img_shape = img_shape #(x, y, channels)\n",
    "        self.architecture = architecture\n",
    "        self.kernel_shape = kernel_shape\n",
    "    \n",
    "    \"\"\"\n",
    "    This function generates lists of weight matrices and bias matrices from\n",
    "    some given architecture.\n",
    "    These can then be used to construct the network.\n",
    "    \"\"\"\n",
    "    def generate_weights_and_biases(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for layer in xrange(self.architecture):\n",
    "            if self.architecture[layer][0] == \"conv\":\n",
    "                if layer == 0: # first layer\n",
    "                    last_output = img_shape[2]\n",
    "                else:\n",
    "                    last_output = self.architecture[layer-1][1]\n",
    "                weights.append(tf.Variable(tf.random_normal([self.kernel_shape[0],\n",
    "                                                            self.kernel_shape[1],\n",
    "                                                            img_shape[2],\n",
    "                                                            self.architecture[layer][1]]\n",
    "                                                           )))\n",
    "                biases.append(tf.Variable(tf.random_normal([self.architecture[layer][1]])))\n",
    "\n",
    "            elif self.architecture[layer][0] == \"dense\":\n",
    "                last_output = self.architecture[layer-1][1]\n",
    "                num_of_conv_layers_so_far = len([i for i in xrange(layer) if self.architecture[i] == \"conv\"])\n",
    "                num_of_input_units = (img_shape[0] * img_shape[1]) / num_of_conv_layers_so_far**2\n",
    "                weights.append(tf.Variable(tf.random_normal([num_of_input_units, self.architecture[layer][1]])))\n",
    "                biases.append(tf.Variable(tf.random_normal([self.architecture[layer][1]])))\n",
    "            \n",
    "            elif self.architecture[layer][0] == \"out\":\n",
    "                last_output = self.architecture[layer-1][1]\n",
    "                weights.append(tf.Variable(tf.random_normal([last_output, self.architecture[layer][1]])))\n",
    "                biases.append(tf.Variable(tf.random_normal([self.architecture[layer][1]])))\n",
    "                \n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \"\"\"\n",
    "    This function creates the graph for training and returns a tensorflow function object.\n",
    "    This object can then be used to train the batches.\n",
    "    \"\"\"\n",
    "    def construct_model(self, batch):\n",
    "        # create variables for layer one\n",
    "        W_conv1 = weight_variable([5, 5, 3, 50])\n",
    "        b_conv1 = bias_variable([50])\n",
    "        \n",
    "        # do convolution\n",
    "        # and max pooling for layer one\n",
    "        h_conv1 = tf.nn.relu(conv2d(batch, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "        # initialize vars for layer two\n",
    "        W_conv2 = weight_variable([5, 5, 50, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "\n",
    "        # convolve and max pool layer 2\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        \n",
    "        # now, do the dense layer 3\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        # apply dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        W_fc2 = weight_variable([1024, 42])\n",
    "        b_fc2 = bias_variable([42])\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "        # add cross entropy as objective function\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        return (train_step, correct_prediction, accuracy)\n",
    "\n",
    "    def train_model(self, images, labels):\n",
    "        print images.shape\n",
    "        x = tf.placeholder(tf.float32, shape=(self.img_shape[0], self.img_shape[1]))\n",
    "        y = tf.placeholder(tf.float32, shape=(42))\n",
    "        \n",
    "        train_op, eval_op, accuracy_op = self.construct_model(x)\n",
    "        x = tf.placeholder(tf.float32, [None, n_input])\n",
    "        keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "        batch_size = 100\n",
    "        batch_runs = images.shape[0] / batch_size\n",
    "        print \"Batch size: \" + str(batch_size)\n",
    "        print \"Number of iterations per epoch: \" + str(batch_runs)\n",
    "        for epoch in range(100):\n",
    "            for batchIdx in range(batch_runs):\n",
    "                train_op.run(feed_dict={x: images[batchIdx*batch_size:(batchIdx+1)*batch_size],\n",
    "                                        y_: labels[batchIdx*batch_size:(batchIdx+1)*batch_size],\n",
    "                                        keep_prob: 0.5})\n",
    "            print \"Epoch \" + str(epoch) + \" done!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 225, 243, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape (225, 243) must have rank 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-0d6863ada53c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-5f4d3b51c077>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, images, labels)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mkeep_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#dropout (keep probability)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-5f4d3b51c077>\u001b[0m in \u001b[0;36mconstruct_model\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# do convolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# and max pooling for layer one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mh_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_conv1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_conv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mh_pool1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_pool_2x2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_conv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-49b9a1e27e47>\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, W)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmax_pool_2x2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sasse/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    293\u001b[0m                               \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                               \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                               data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sasse/.local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    653\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    654\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    656\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_Restructure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sasse/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2154\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2155\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2156\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2157\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sasse/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1610\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[0;32m   1611\u001b[0m                          % op.type)\n\u001b[1;32m-> 1612\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1613\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m/home/sasse/.local/lib/python2.7/site-packages/tensorflow/python/ops/common_shapes.pyc\u001b[0m in \u001b[0;36mconv2d_shape\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mare\u001b[0m \u001b[0mincompatible\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m   \"\"\"\n\u001b[1;32m--> 202\u001b[1;33m   \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m   \u001b[0mfilter_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sasse/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mwith_rank\u001b[1;34m(self, rank)\u001b[0m\n\u001b[0;32m    623\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munknown_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shape %s must have rank %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwith_rank_at_least\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape (225, 243) must have rank 4"
     ]
    }
   ],
   "source": [
    "architecture = [(\"conv\", 32), (\"conv\", 64), (\"dense\", 1024), (\"out\", 42)]\n",
    "img_shape = maxShape\n",
    "kernel_shape = (5, 5)\n",
    "learning_rate = 0.5\n",
    "classifier = cNN(architecture, img_shape, kernel_shape, learning_rate)\n",
    "classifier.train_model(np.array(train_set), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
